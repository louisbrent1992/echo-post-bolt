import 'dart:async';
import 'dart:io';
import 'dart:convert';
import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';
import 'package:provider/provider.dart';
import 'package:http/http.dart' as http;
import 'package:flutter_dotenv/flutter_dotenv.dart';
import 'package:record/record.dart';
import 'package:path_provider/path_provider.dart';

import '../models/social_action.dart';
import '../services/ai_service.dart';
import '../services/media_coordinator.dart';
import '../widgets/mic_button.dart';
import '../widgets/social_icon.dart';
import '../widgets/post_preview.dart';
import '../widgets/post_content_box.dart';
import '../widgets/transcription_status.dart';
import '../widgets/unified_action_button.dart';
import '../widgets/directory_selector.dart';
import '../screens/media_selection_screen.dart';
import '../screens/review_post_screen.dart';
import '../screens/history_screen.dart';
import '../services/firestore_service.dart';

/// EchoPost Voice-to-Post Pipeline
///
/// This screen orchestrates the end-to-end workflow that turns a user's spoken command into a fully-formed social-media post:
///
/// 1. Voice Capture ‚Äì When the user taps the microphone, the `record` package
///    begins capturing audio in M4A format for optimal file size.
/// 2. Transcription ‚Äì Once stopped, the finalized M4A file is shipped to
///    OpenAI Whisper (`/v1/audio/transcriptions`) where it is transcribed
///    into plain-text English.
/// 3. Command Parsing ‚Äì The raw transcript is forwarded to ChatGPT (`/v1/chat/completions`).
///    Using a strict JSON schema, ChatGPT returns a `SocialAction` description
///    that contains every field needed to publish a post: text, hashtags,
///    mentions, media placeholders, platform targets, scheduling info, etc.
/// 4. Persistence ‚Äì The resulting `SocialAction` is persisted to Firestore via
///    `FirestoreService.saveAction` so that drafts and history survive app
///    restarts or network failures.
/// 5. Media Resolution ‚Äì If the JSON includes a `media_query`, the user is
///    routed to `MediaSelectionScreen` to pick matching local assets.  If the
///    JSON already contains concrete `media.file_uri` entries, this step is
///    skipped. This is now coordinated through `MediaCoordinator` for consistent
///    validation, metadata enrichment, and recovery mechanisms.
/// 6. Review & Publish ‚Äì Finally, the user lands on `ReviewPostScreen`,
///    previews the composed post, edits if necessary, and confirms.  Upon
///    confirmation, the background posting workflow dispatches the post to the
///    respective social-media APIs once account authentication is verified.
///
/// Each stage contains robust validation, error handling, and debug logging so
/// that any break in the pipeline (Whisper outages, malformed JSON, network
/// errors, etc.) is surfaced early and the UI gracefully degrades.  Keep this
/// contract intact whenever UI or service changes are introduced.

class CommandScreen extends StatefulWidget {
  const CommandScreen({super.key});

  @override
  State<CommandScreen> createState() => _CommandScreenState();
}

class _CommandScreenState extends State<CommandScreen>
    with TickerProviderStateMixin {
  final AudioRecorder _record = AudioRecorder();

  RecordingState _recordingState = RecordingState.idle;
  String _transcription = '';
  SocialAction? _currentAction;
  String? _currentRecordingPath;
  bool _isStoppingRecording = false;
  bool _isStartingRecording = false;
  DateTime? _recordingStartTime;

  late AnimationController _backgroundController;
  late Animation<double> _backgroundAnimation;

  Timer? _recordingTimer;
  Timer? _amplitudeTimer;
  int _recordingDuration = 0;
  final int _maxRecordingDuration = 30; // 30 seconds max

  // Voice monitoring variables
  double _currentAmplitude = -160.0; // dBFS
  double _maxAmplitude = -160.0;
  double _amplitudeSum = 0.0;
  int _amplitudeSamples = 0;
  bool _hasSpeechDetected = false;
  int _silenceCount = 0;
  final double _speechThreshold = -40.0; // dBFS threshold for speech detection
  final double _silenceThreshold = -50.0; // dBFS threshold for silence
  final int _maxSilenceBeforeWarning =
      10; // 5 seconds of silence before warning

  // Media coordinator for centralized media handling
  late final MediaCoordinator _mediaCoordinator;

  @override
  void initState() {
    super.initState();
    _initializeAnimations();
  }

  @override
  void didChangeDependencies() {
    super.didChangeDependencies();
    _mediaCoordinator = Provider.of<MediaCoordinator>(context, listen: false);
  }

  void _initializeAnimations() {
    _backgroundController = AnimationController(
      duration: const Duration(seconds: 10),
      vsync: this,
    );

    _backgroundAnimation = Tween<double>(
      begin: 0.0,
      end: 1.0,
    ).animate(CurvedAnimation(
      parent: _backgroundController,
      curve: Curves.easeInOut,
    ));

    _backgroundController.repeat(reverse: true);
  }

  @override
  void dispose() {
    _backgroundController.dispose();
    _recordingTimer?.cancel();
    _amplitudeTimer?.cancel();
    _record.dispose();
    super.dispose();
  }

  void _resetRecordingState() {
    _recordingTimer?.cancel();
    _amplitudeTimer?.cancel();
    _recordingTimer = null;
    _amplitudeTimer = null;
    _recordingDuration = 0;
    _transcription = '';
    _currentAction = null;
    _currentRecordingPath = null;
    _isStartingRecording = false;
    _isStoppingRecording = false;
    _recordingState = RecordingState.idle;
    _recordingStartTime = null;

    // Reset voice monitoring
    _currentAmplitude = -160.0;
    _maxAmplitude = -160.0;
    _amplitudeSum = 0.0;
    _amplitudeSamples = 0;
    _hasSpeechDetected = false;
    _silenceCount = 0;

    if (kDebugMode) {
      print('üîÑ Recording state reset to idle');
    }
  }

  double get normalizedAmplitude {
    // Convert dBFS to 0.0-1.0 range for UI
    // -60 dBFS = 0.0, -10 dBFS = 1.0
    const double minDb = -60.0;
    const double maxDb = -10.0;

    if (_currentAmplitude <= minDb) return 0.0;
    if (_currentAmplitude >= maxDb) return 1.0;

    return (_currentAmplitude - minDb) / (maxDb - minDb);
  }

  Future<void> _startRecording() async {
    if (kDebugMode) {
      print(
          'üé§ Starting recording... Current state: $_recordingState, isStartingRecording: $_isStartingRecording');
    }

    if (_recordingState != RecordingState.idle) {
      if (kDebugMode) {
        print('‚ùå Cannot start recording: not in idle state');
      }
      return;
    }

    if (_isStartingRecording) {
      if (kDebugMode) {
        print(
            '‚ùå Already starting recording, ignoring additional start request');
      }
      return;
    }

    _isStartingRecording = true;

    try {
      // Check and request microphone permission
      if (kDebugMode) {
        print('üîç Checking microphone permission...');
      }

      bool hasPermission = await _record.hasPermission();
      if (kDebugMode) {
        print('üîç Initial microphone permission: $hasPermission');
      }

      // Final permission check - the record package automatically requests permission if needed
      if (!hasPermission) {
        _isStartingRecording = false;
        if (kDebugMode) {
          print('‚ùå Microphone permission denied by user');
        }
        if (mounted) {
          ScaffoldMessenger.of(context).showSnackBar(
            SnackBar(
              content: const Text(
                  'Microphone permission is required for voice recording. Please grant permission when prompted or check app settings.'),
              duration: const Duration(seconds: 5),
              action: SnackBarAction(
                label: 'Retry',
                onPressed: () {
                  // Retry recording which will trigger permission request again
                  _startRecording();
                },
              ),
            ),
          );
        }
        return;
      }

      if (kDebugMode) {
        print('‚úÖ Microphone permission granted');
      }

      // Validate recording environment
      if (kDebugMode) {
        print('üîç Validating recording environment...');
      }

      if (!await _validateRecordingEnvironment()) {
        _isStartingRecording = false;
        if (kDebugMode) {
          print('‚ùå Recording environment validation failed');
        }
        return;
      }

      // Set up recording path - use M4A format for optimal file size
      final tempDir = await getTemporaryDirectory();

      // Ensure temp directory exists and is writable
      if (!await tempDir.exists()) {
        try {
          await tempDir.create(recursive: true);
          if (kDebugMode) {
            print('üìÅ Created temp directory: ${tempDir.path}');
          }
        } catch (e) {
          throw Exception('Failed to create temporary directory: $e');
        }
      }

      // Use M4A format for optimal file size
      final m4aPath =
          '${tempDir.path}/recording_${DateTime.now().millisecondsSinceEpoch}.m4a';

      if (kDebugMode) {
        print('üìÅ Recording to path: $m4aPath');
        print('üìÅ Temp directory exists: ${await tempDir.exists()}');
        print('üìÅ Temp directory path: ${tempDir.path}');
      }

      // Set the path BEFORE starting recording
      _currentRecordingPath = m4aPath;

      // Update UI state immediately
      setState(() {
        _recordingState = RecordingState.recording;
      });

      if (kDebugMode) {
        print('üéõÔ∏è UI state updated to recording');
      }

      // Configure optimal recording settings for speech recognition
      // Using M4A format for optimal file size
      const recordConfig = RecordConfig(
        encoder: AudioEncoder.aacLc, // AAC-LC codec for M4A format
        bitRate: 128000, // 128 kbps - good quality for speech
        sampleRate:
            16000, // 16kHz - optimal for speech recognition (Whisper's native rate)
        numChannels: 1, // Mono recording for voice
        autoGain: true, // Enable auto gain for consistent levels
        echoCancel: true, // Enable echo cancellation
        noiseSuppress: true, // Enable noise suppression
      );

      if (kDebugMode) {
        print('üéõÔ∏è Recording config:');
        print('   Encoder: ${recordConfig.encoder}');
        print('   Bitrate: ${recordConfig.bitRate} bps');
        print('   Sample Rate: ${recordConfig.sampleRate} Hz');
        print('   Channels: ${recordConfig.numChannels}');
        print('   Auto Gain: ${recordConfig.autoGain}');
        print('   Echo Cancel: ${recordConfig.echoCancel}');
        print('   Noise Suppress: ${recordConfig.noiseSuppress}');
      }

      // Start the actual recording
      if (kDebugMode) {
        print('üöÄ Starting audio recording...');
      }

      _recordingStartTime = DateTime.now();
      await _record.start(recordConfig, path: m4aPath);

      if (kDebugMode) {
        print(
            'üéôÔ∏è Audio recording started at: ${_recordingStartTime!.toIso8601String()}');
      }

      // Small delay to allow microphone to fully initialize before starting timer
      // This prevents timing mismatch between timer and actual audio capture
      await Future.delayed(const Duration(milliseconds: 100));

      // Start the recording timer and amplitude monitoring
      _startRecordingTimer();
      _startAmplitudeMonitoring();

      if (kDebugMode) {
        print('‚úÖ Recording setup complete. Path: $_currentRecordingPath');
        print('‚è∞ Recording timer and amplitude monitoring started');
      }
    } catch (e) {
      if (kDebugMode) {
        print('‚ùå Failed to start recording: $e');
        print('üìä Error type: ${e.runtimeType}');
      }

      // Reset state on error
      setState(() {
        _recordingState = RecordingState.idle;
      });
      _currentRecordingPath = null;

      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(
            content: Text('Failed to start recording: $e'),
            duration: const Duration(seconds: 4),
          ),
        );
      }
    } finally {
      _isStartingRecording = false;
      if (kDebugMode) {
        print('üîö _startRecording completed. Final state: $_recordingState');
      }
    }
  }

  void _startAmplitudeMonitoring() {
    _currentAmplitude = -160.0;
    _maxAmplitude = -160.0;
    _amplitudeSum = 0.0;
    _amplitudeSamples = 0;
    _hasSpeechDetected = false;
    _silenceCount = 0;

    if (kDebugMode) {
      print('üîä Starting amplitude monitoring...');
    }

    _amplitudeTimer =
        Timer.periodic(const Duration(milliseconds: 500), (timer) async {
      try {
        final amplitude = await _record.getAmplitude();
        final current = amplitude.current;

        setState(() {
          _currentAmplitude = current;

          if (current > _maxAmplitude) {
            _maxAmplitude = current;
          }

          _amplitudeSamples++;
          _amplitudeSum += current;

          // Check for speech detection
          if (current > _speechThreshold) {
            _hasSpeechDetected = true;
            _silenceCount = 0;
          } else if (current < _silenceThreshold) {
            _silenceCount++;
          }
        });

        if (kDebugMode) {
          print(
              'üîä Amplitude: ${current.toStringAsFixed(1)} dBFS (Max: ${_maxAmplitude.toStringAsFixed(1)}, Normalized: ${normalizedAmplitude.toStringAsFixed(2)})');

          // Warn about potential issues
          if (current < -50.0 && _amplitudeSamples > 4) {
            print(
                '‚ö†Ô∏è Low audio level detected. Speak louder or closer to microphone.');
          }
        }

        // Check for extended silence during recording
        if (_silenceCount >= _maxSilenceBeforeWarning && !_hasSpeechDetected) {
          if (kDebugMode) {
            print('‚ö†Ô∏è Extended silence detected - user may not be speaking');
          }

          if (mounted) {
            ScaffoldMessenger.of(context).showSnackBar(
              const SnackBar(
                content: Text(
                    'üé§ Speak closer to the microphone - no voice detected'),
                duration: Duration(seconds: 2),
                backgroundColor: Colors.orange,
              ),
            );
          }
        }
      } catch (e) {
        if (kDebugMode) {
          print('‚ùå Error getting amplitude: $e');
        }
      }
    });
  }

  void _stopAmplitudeMonitoring() {
    _amplitudeTimer?.cancel();
    _amplitudeTimer = null;

    if (kDebugMode) {
      final averageAmplitude =
          _amplitudeSamples > 0 ? _amplitudeSum / _amplitudeSamples : -160.0;

      print('üîä Final audio analysis:');
      print('   Max amplitude: ${_maxAmplitude.toStringAsFixed(1)} dBFS');
      print(
          '   Average amplitude: ${averageAmplitude.toStringAsFixed(1)} dBFS');
      print('   Speech detected: $_hasSpeechDetected');
      print('   Samples: $_amplitudeSamples');

      // Provide audio quality assessment
      if (!_hasSpeechDetected) {
        print('‚ùå CRITICAL: No speech detected throughout recording');
        print('   This explains why Whisper returns very short transcriptions');
        print(
            '   Recommendation: Speak louder, closer to microphone, or check microphone permissions');
      } else if (_maxAmplitude < -30.0) {
        print('‚ö†Ô∏è WARNING: Low speech levels detected');
        print(
            '   Recommendation: Speak louder or closer to microphone for better accuracy');
      } else {
        print('‚úÖ Good speech levels detected');
      }
    }
  }

  Future<void> _stopRecording() async {
    final stopTime = DateTime.now();
    if (kDebugMode) {
      print('üõë Stopping recording at: ${stopTime.toIso8601String()}');
      print(
          'üõë Current state: $_recordingState, isStoppingRecording: $_isStoppingRecording');
      print('üõë Recording duration when stop called: ${_recordingDuration}s');
    }

    // Handle different states
    if (_recordingState == RecordingState.idle) {
      if (kDebugMode) {
        print('‚ùå Already in idle state, nothing to stop');
      }
      return;
    }

    // If we're in processing or ready state, just reset to idle
    if (_recordingState == RecordingState.processing ||
        _recordingState == RecordingState.ready) {
      if (kDebugMode) {
        print('üîÑ Resetting from $_recordingState to idle');
      }
      setState(() {
        _resetRecordingState();
      });
      return;
    }

    // Handle the actual recording stop (only when in recording state)
    if (_recordingState != RecordingState.recording) {
      if (kDebugMode) {
        print(
            '‚ùå Cannot stop recording: not in recording state (current: $_recordingState)');
      }
      return;
    }

    if (_isStoppingRecording) {
      if (kDebugMode) {
        print('‚ùå Already stopping recording, ignoring additional stop request');
      }
      return;
    }

    _isStoppingRecording = true;

    try {
      if (kDebugMode) {
        print('üìç About to call _record.stop()');
      }

      // Stop the recording and get the path
      final recordedPath = await _record.stop();
      _stopRecordingTimer();
      _stopAmplitudeMonitoring();

      final actualDuration = _recordingStartTime != null
          ? stopTime.difference(_recordingStartTime!).inMilliseconds / 1000.0
          : 0.0;

      if (kDebugMode) {
        print('üìç Recording stopped, returned path: $recordedPath');
        print('üìç Current recording path: $_currentRecordingPath');
        print('üìç Timer duration: ${_recordingDuration}s');
        print('üìç Actual duration: ${actualDuration.toStringAsFixed(2)}s');
      }

      // Check if speech was detected during recording
      if (!_hasSpeechDetected) {
        if (mounted) {
          ScaffoldMessenger.of(context).showSnackBar(
            const SnackBar(
              content: Text(
                  'üé§ No speech detected. Try speaking louder, closer to the microphone, or check your microphone permissions.'),
              duration: Duration(seconds: 4),
              backgroundColor: Colors.orange,
            ),
          );
        }
        throw Exception(
            'No speech detected during recording. Please speak louder, closer to the microphone, or check microphone permissions.');
      }

      // Check minimum recording duration (0.5 second minimum for meaningful speech)
      const minRecordingDuration = 0.5;
      if (actualDuration < minRecordingDuration) {
        throw Exception(
            'Recording too short (${actualDuration.toStringAsFixed(2)}s). Please hold the button longer and speak clearly. Minimum duration: ${minRecordingDuration}s');
      }

      setState(() {
        _recordingState = RecordingState.processing;
      });

      // Use the returned path or fallback to stored path
      final pathToProcess = recordedPath ?? _currentRecordingPath;

      if (pathToProcess == null || pathToProcess.isEmpty) {
        throw Exception('No recording path available');
      }

      // Verify file exists before processing
      final file = File(pathToProcess);
      if (!await file.exists()) {
        throw Exception('Recording file does not exist: $pathToProcess');
      }

      final fileSize = await file.length();
      if (fileSize == 0) {
        throw Exception('Recording file is empty');
      }

      // Validate M4A file format
      await _validateM4aFile(file, fileSize);

      if (kDebugMode) {
        print('‚è≥ Processing recording from: $pathToProcess ($fileSize bytes)');
      }

      final transcription = await _transcribeWithWhisper(pathToProcess);

      if (transcription.isEmpty) {
        throw Exception('Whisper returned empty transcription');
      }

      await _processTranscription(transcription);
    } catch (e) {
      if (kDebugMode) {
        print('‚ùå Error in _stopRecording: $e');
      }
      setState(() {
        _recordingState = RecordingState.idle;
      });
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(
            content: Text('Failed to process recording: $e'),
            duration: const Duration(seconds: 5),
          ),
        );
      }
    } finally {
      _isStoppingRecording = false;
    }
  }

  void _startRecordingTimer() {
    _recordingDuration = 0;
    if (kDebugMode) {
      print('‚è∞ Starting recording timer...');
    }

    _recordingTimer = Timer.periodic(const Duration(seconds: 1), (timer) {
      setState(() {
        _recordingDuration++;
      });

      if (kDebugMode) {
        print(
            '‚è∞ Recording duration: ${_recordingDuration}s / ${_maxRecordingDuration}s');
      }

      if (_recordingDuration >= _maxRecordingDuration) {
        if (kDebugMode) {
          print('‚è∞ Max recording duration reached, stopping...');
        }
        _stopRecording();
      }
    });
  }

  void _stopRecordingTimer() {
    if (kDebugMode) {
      print(
          '‚è∞ Stopping recording timer. Final duration: ${_recordingDuration}s');
    }
    _recordingTimer?.cancel();
    _recordingTimer = null;
  }

  /// Validates that the recorded file is a proper M4A format and contains audio data
  Future<void> _validateM4aFile(File file, int fileSize) async {
    try {
      // Check minimum file size (M4A header is typically 32+ bytes)
      if (fileSize < 100) {
        throw Exception(
            'M4A file too small ($fileSize bytes), likely corrupted or incomplete');
      }

      // Check reasonable maximum size for a 30-second recording
      // At 128kbps, 30 seconds should be roughly 480KB, so anything over 5MB is suspicious
      if (fileSize > 5 * 1024 * 1024) {
        if (kDebugMode) {
          print(
              '‚ö†Ô∏è Warning: M4A file unusually large (${(fileSize / 1024 / 1024).toStringAsFixed(2)} MB)');
        }
      }

      // Read first 32 bytes to check M4A header
      final bytes = await file.openRead(0, 32).toList();
      final headerBytes = bytes.expand((x) => x).toList();

      if (headerBytes.length < 32) {
        throw Exception(
            'Unable to read M4A file header (got ${headerBytes.length} bytes)');
      }

      // Check for M4A/MP4 container signature (ftyp box)
      // Bytes 4-7 should contain 'ftyp'
      final ftypSignature = String.fromCharCodes(headerBytes.sublist(4, 8));
      if (ftypSignature != 'ftyp') {
        if (kDebugMode) {
          print('‚ö†Ô∏è Warning: Expected ftyp signature, got: $ftypSignature');
          print('   File may still be valid M4A, continuing...');
        }
      }

      if (kDebugMode) {
        print('‚úÖ M4A file validation passed:');
        print(
            '   File size: $fileSize bytes (${(fileSize / 1024).toStringAsFixed(1)} KB)');
        print('   Header signature: $ftypSignature');
        print('   Speech detected during recording: $_hasSpeechDetected');
        print('   Max amplitude: ${_maxAmplitude.toStringAsFixed(1)} dBFS');
      }
    } catch (e) {
      if (kDebugMode) {
        print('‚ùå M4A validation error: $e');
      }
      throw Exception('M4A file validation failed: $e');
    }
  }

  /// Validates the recording environment before starting
  Future<bool> _validateRecordingEnvironment() async {
    try {
      if (kDebugMode) {
        print('üîç Validating recording environment...');
      }

      // Check if we can access the temp directory
      final tempDir = await getTemporaryDirectory();

      if (kDebugMode) {
        print('üìÅ Temp directory: ${tempDir.path}');
      }

      // Verify directory exists and is accessible
      final dirStat = await tempDir.stat();
      final dirExists = dirStat.type != FileSystemEntityType.notFound;

      if (kDebugMode) {
        print('üìÅ Directory exists: $dirExists');
      }

      if (!dirExists) {
        throw Exception('Temp directory does not exist');
      }

      // Check if temp directory is writable by trying to create a test file
      try {
        final testFile = File(
            '${tempDir.path}/test_${DateTime.now().millisecondsSinceEpoch}.tmp');

        if (kDebugMode) {
          print('üß™ Testing write access with file: ${testFile.path}');
        }

        await testFile.writeAsBytes([1, 2, 3, 4]);

        // Verify file was created
        final exists = await testFile.exists();
        if (!exists) {
          throw Exception('Test file was not created');
        }

        // Clean up test file
        await testFile.delete();

        if (kDebugMode) {
          print('‚úÖ Temp directory is writable');
        }
      } catch (e) {
        if (kDebugMode) {
          print('‚ùå Temp directory not writable: $e');
        }
        if (mounted) {
          ScaffoldMessenger.of(context).showSnackBar(
            const SnackBar(
              content: Text(
                  'Cannot access storage for recording. Please check app permissions.'),
              duration: Duration(seconds: 4),
            ),
          );
        }
        return false;
      }

      if (kDebugMode) {
        print('‚úÖ Recording environment validation passed');
      }

      return true;
    } catch (e) {
      if (kDebugMode) {
        print('‚ùå Recording environment validation failed: $e');
      }

      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(
            content: Text('Recording environment check failed: $e'),
            duration: const Duration(seconds: 4),
          ),
        );
      }

      return false;
    }
  }

  /// Validates that the SocialAction created from ChatGPT response is complete and ready for UI
  void _validateSocialAction(
      SocialAction action, String originalTranscription) {
    try {
      // Validate basic structure
      if (action.actionId.isEmpty) {
        throw Exception('SocialAction missing actionId');
      }

      if (action.platforms.isEmpty) {
        throw Exception('SocialAction has no platforms selected');
      }

      if (action.content.text.isEmpty) {
        throw Exception('SocialAction has empty text content');
      }

      // Validate platform consistency
      final validPlatforms = ['instagram', 'twitter', 'facebook', 'tiktok'];
      for (final platform in action.platforms) {
        if (!validPlatforms.contains(platform)) {
          throw Exception('Invalid platform: $platform');
        }
      }

      // Validate content structure
      if (action.content.hashtags.any((tag) => tag.startsWith('#'))) {
        throw Exception('Hashtags should not include # symbol');
      }

      // Validate internal metadata
      if (action.internal.originalTranscription != originalTranscription) {
        if (kDebugMode) {
          print('‚ö†Ô∏è Warning: Original transcription mismatch');
          print('   Expected: "$originalTranscription"');
          print('   Stored: "${action.internal.originalTranscription}"');
        }
      }

      if (!action.internal.aiGenerated) {
        throw Exception('SocialAction should be marked as AI generated');
      }

      // Log validation success
      if (kDebugMode) {
        print('‚úÖ SocialAction validation passed:');
        print('   ID: ${action.actionId}');
        print(
            '   Platforms: ${action.platforms.length} (${action.platforms.join(', ')})');
        print('   Text: ${action.content.text.length} chars');
        print('   Hashtags: ${action.content.hashtags.length}');
        print('   Mentions: ${action.content.mentions.length}');
        print(
            '   Media Query: ${action.mediaQuery?.isNotEmpty == true ? 'Yes' : 'No'}');
        print('   Created: ${action.createdAt}');
      }
    } catch (e) {
      if (kDebugMode) {
        print('‚ùå SocialAction validation failed: $e');
        print('üìä Action JSON: ${action.toJson()}');
      }
      throw Exception('ChatGPT response validation failed: $e');
    }
  }

  Future<String> _transcribeWithWhisper(String audioPath) async {
    final apiKey = dotenv.env['OPENAI_API_KEY'];
    if (apiKey == null || apiKey.isEmpty) {
      throw Exception(
          'OPENAI_API_KEY not found in .env.local file. Please add your OpenAI API key.');
    }

    if (kDebugMode) {
      print('üéµ Transcribing M4A audio file: $audioPath');
    }

    try {
      // Check if file exists and validate
      final file = File(audioPath);
      if (!await file.exists()) {
        throw Exception('Audio file does not exist: $audioPath');
      }

      final fileSize = await file.length();

      // Check file size limit (25MB = 26,214,400 bytes)
      const maxFileSize = 26214400;
      if (fileSize > maxFileSize) {
        throw Exception(
            'Audio file too large: $fileSize bytes. Maximum allowed: $maxFileSize bytes (25MB)');
      }

      if (fileSize == 0) {
        throw Exception('Audio file is empty');
      }

      if (kDebugMode) {
        print(
            'üìÅ File size: $fileSize bytes (${(fileSize / 1024 / 1024).toStringAsFixed(2)} MB)');
      }

      final url = Uri.parse('https://api.openai.com/v1/audio/transcriptions');
      final request = http.MultipartRequest('POST', url);

      // Set headers
      request.headers['Authorization'] = 'Bearer $apiKey';
      request.headers['User-Agent'] = 'EchoPost/1.0.0';

      // Set required fields
      request.fields['model'] = 'whisper-1';
      request.fields['response_format'] = 'json';

      // Optional: specify language for better accuracy
      request.fields['language'] = 'en';

      // Optional: lower temperature for more deterministic results
      request.fields['temperature'] = '0.0';

      // Add the audio file
      final multipartFile = await http.MultipartFile.fromPath(
        'file',
        audioPath,
        filename: 'audio.m4a',
      );
      request.files.add(multipartFile);

      if (kDebugMode) {
        print('üöÄ Sending request to Whisper API...');
        print('üìã Request fields: ${request.fields}');
        print(
            'üìé File: ${multipartFile.filename} (${multipartFile.length} bytes)');
      }

      // Send request with timeout
      final response = await request.send().timeout(
        const Duration(seconds: 60), // 60 second timeout for audio processing
        onTimeout: () {
          throw Exception('Whisper API request timed out after 60 seconds');
        },
      );

      final responseBody = await response.stream.bytesToString();

      if (kDebugMode) {
        print('üì° Whisper API response status: ${response.statusCode}');
        print('üì° Whisper API response headers: ${response.headers}');
      }

      if (response.statusCode != 200) {
        String errorMessage = 'Whisper API error (${response.statusCode})';

        try {
          final errorJson = json.decode(responseBody);
          if (errorJson.containsKey('error')) {
            final errorDetails = errorJson['error'];
            errorMessage +=
                ': ${errorDetails['message'] ?? errorDetails['type'] ?? 'Unknown error'}';

            if (errorDetails.containsKey('code')) {
              errorMessage += ' (Code: ${errorDetails['code']})';
            }
          }
        } catch (e) {
          errorMessage += ': $responseBody';
        }

        throw Exception(errorMessage);
      }

      if (kDebugMode) {
        print('üì° Whisper API response body: $responseBody');
      }

      // Parse JSON response
      final jsonResponse = json.decode(responseBody) as Map<String, dynamic>;

      if (!jsonResponse.containsKey('text')) {
        throw Exception(
            'Invalid response from Whisper API: missing "text" field');
      }

      final transcription = jsonResponse['text'] as String? ?? '';

      if (transcription.isEmpty) {
        throw Exception('Whisper API returned empty transcription');
      }

      if (kDebugMode) {
        print('‚úÖ Transcription received: "$transcription"');
      }

      // Clean up the audio file
      try {
        await file.delete();
        if (kDebugMode) {
          print('üóëÔ∏è Cleaned up audio file');
        }
      } catch (e) {
        if (kDebugMode) {
          print('‚ö†Ô∏è Warning: Could not delete audio file: $e');
        }
      }

      return transcription.trim();
    } catch (e) {
      if (kDebugMode) {
        print('‚ùå Whisper transcription error: $e');
      }

      // Clean up the audio file even on error
      try {
        await File(audioPath).delete();
      } catch (_) {
        // Ignore cleanup errors
      }

      rethrow;
    }
  }

  Future<void> _processTranscription(String transcription) async {
    if (kDebugMode) {
      print('üîÑ Processing transcription: "$transcription"');
    }

    try {
      if (!mounted) return;

      final aiService = Provider.of<AIService>(context, listen: false);
      final firestoreService =
          Provider.of<FirestoreService>(context, listen: false);

      if (kDebugMode) {
        print('üöÄ Calling AIService.processVoiceCommand...');
      }

      final action = await aiService.processVoiceCommand(transcription);

      if (kDebugMode) {
        print('‚úÖ Received SocialAction from AI service');
        print('üìã Platforms: ${action.platforms}');
        print('üìù Text: "${action.content.text}"');
        print('üè∑Ô∏è Hashtags: ${action.content.hashtags}');
        print('üîç Media Query: "${action.mediaQuery}"');
      }

      // Validate the created SocialAction
      _validateSocialAction(action, transcription);

      // Save the action to Firestore
      try {
        await firestoreService.saveAction(action.toJson());
        if (kDebugMode) {
          print('üíæ Saved action to Firestore');
        }
      } catch (e) {
        if (kDebugMode) {
          print('‚ö†Ô∏è Failed to save to Firestore: $e');
        }
        // Continue even if Firestore save fails
      }

      // Use MediaCoordinator to recover and validate media state
      if (kDebugMode) {
        print('üîÑ Using MediaCoordinator to recover/validate media state...');
      }

      final recoveredAction = await _mediaCoordinator.recoverMediaState(action);
      final finalAction = recoveredAction ?? action;

      if (mounted) {
        setState(() {
          _transcription = transcription;
          _currentAction = finalAction;
          _recordingState = RecordingState.ready;
        });

        // Enhanced media status logging using MediaCoordinator
        if (kDebugMode) {
          print('üéØ UI updated with validated action');

          // Check media status through coordinator
          final hasValidMedia = finalAction.content.media.isNotEmpty &&
              finalAction.content.media
                  .any((media) => media.fileUri.isNotEmpty);
          final hasMediaQuery =
              finalAction.mediaQuery?.searchTerms.isNotEmpty == true;

          if (hasValidMedia) {
            // Validate existing media URIs
            var validCount = 0;
            for (final media in finalAction.content.media) {
              if (await _mediaCoordinator.validateMediaURI(media.fileUri)) {
                validCount++;
              }
            }
            print(
                '‚úÖ Action contains $validCount valid media file URIs out of ${finalAction.content.media.length}');
          } else if (hasMediaQuery) {
            print('üîç Action contains media query - will need media selection');
            print(
                '   Search Terms: "${finalAction.mediaQuery!.searchTerms.join(', ')}"');
            print('   Media Types: ${finalAction.mediaQuery!.mediaTypes}');
          } else {
            print('‚ö†Ô∏è Action has no media or media query');
          }
        }
      }
    } catch (e) {
      if (kDebugMode) {
        print('‚ùå Error in _processTranscription: $e');
        print('üìä Stack trace: ${StackTrace.current}');
      }

      if (mounted) {
        setState(() {
          _recordingState = RecordingState.idle;
        });

        // Show more detailed error message
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(
            content: Text('Failed to process transcription: $e'),
            duration: const Duration(seconds: 5),
            action: SnackBarAction(
              label: 'Retry',
              onPressed: () {
                if (_transcription.isNotEmpty) {
                  _processTranscription(_transcription);
                }
              },
            ),
          ),
        );
      }
    }
  }

  void _togglePlatform(String platform) {
    if (_currentAction == null) return;

    final updatedPlatforms = List<String>.from(_currentAction!.platforms);
    if (updatedPlatforms.contains(platform)) {
      updatedPlatforms.remove(platform);
    } else {
      updatedPlatforms.add(platform);
    }

    setState(() {
      _currentAction = SocialAction(
        actionId: _currentAction!.actionId,
        createdAt: _currentAction!.createdAt,
        platforms: updatedPlatforms,
        content: _currentAction!.content,
        options: _currentAction!.options,
        platformData: _currentAction!.platformData,
        internal: _currentAction!.internal,
        mediaQuery: _currentAction!.mediaQuery,
      );
    });
  }

  Future<void> _navigateToMediaSelection() async {
    if (_currentAction == null) return;

    try {
      if (kDebugMode) {
        print('üîç Navigating to media selection with MediaCoordinator...');
      }

      // Fetch candidates via MediaCoordinator if we have a query
      List<Map<String, dynamic>>? initialCandidates;
      if (_currentAction!.mediaQuery != null) {
        final query = _currentAction!.mediaQuery!;
        final searchTerms = query.searchTerms.join(' ');

        // Build date range if available
        DateTimeRange? dateRange;
        if (query.dateRange != null) {
          dateRange = DateTimeRange(
            start: query.dateRange!.startDate ??
                DateTime.now().subtract(const Duration(days: 365)),
            end: query.dateRange!.endDate ?? DateTime.now(),
          );
        }

        if (kDebugMode) {
          print('   Search Terms: "$searchTerms"');
          print('   Media Types: ${query.mediaTypes}');
          print('   Date Range: ${dateRange?.toString() ?? 'None'}');
          print('   Directory: ${query.directoryPath ?? 'All'}');
        }

        // Get media candidates through coordinator
        initialCandidates = await _mediaCoordinator.getMediaForQuery(
          searchTerms,
          dateRange: dateRange,
          mediaTypes: query.mediaTypes.isNotEmpty ? query.mediaTypes : null,
          directory: query.directoryPath,
        );

        if (kDebugMode) {
          print('üìä Found ${initialCandidates.length} media candidates');
        }
      }

      final updatedAction = await Navigator.push<SocialAction>(
        context,
        MaterialPageRoute(
          builder: (context) => MediaSelectionScreen(
            action: _currentAction!,
            initialCandidates: initialCandidates,
          ),
        ),
      );

      if (updatedAction != null) {
        // Validate the updated action through coordinator before setting state
        final validatedAction =
            await _mediaCoordinator.recoverMediaState(updatedAction);
        setState(() {
          _currentAction = validatedAction ?? updatedAction;
        });

        if (kDebugMode) {
          print('‚úÖ Media selection completed and validated');
          print(
              'üìä Selected media count: ${_currentAction!.content.media.length}');
        }
      }
    } catch (e) {
      if (kDebugMode) {
        print('‚ùå Error in media selection navigation: $e');
      }

      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(
            content: Text('Failed to load media selection: $e'),
            duration: const Duration(seconds: 3),
          ),
        );
      }
    }
  }

  Future<void> _navigateToReviewPost() async {
    if (_currentAction == null) return;

    try {
      if (kDebugMode) {
        print('üîç Navigating to review post with final validation...');
      }

      // Last-minute sanity check through MediaCoordinator
      final safeAction =
          await _mediaCoordinator.recoverMediaState(_currentAction!);
      final actionToReview = safeAction ?? _currentAction!;

      if (kDebugMode) {
        print('‚úÖ Final action validated for review');
        print('üìä Media count: ${actionToReview.content.media.length}');

        // Log any media validation issues
        if (actionToReview.content.media.isNotEmpty) {
          var validCount = 0;
          for (final media in actionToReview.content.media) {
            if (await _mediaCoordinator.validateMediaURI(media.fileUri)) {
              validCount++;
            }
          }
          print(
              'üìä Valid media URIs: $validCount/${actionToReview.content.media.length}');
        }
      }

      final result = await Navigator.push(
        context,
        MaterialPageRoute(
          builder: (context) => ReviewPostScreen(action: actionToReview),
        ),
      );

      if (result == true) {
        // Post was successfully published, reset the state
        if (kDebugMode) {
          print('‚úÖ Post published successfully, resetting state');
        }
        setState(() {
          _resetRecordingState();
        });
      }
    } catch (e) {
      if (kDebugMode) {
        print('‚ùå Error in review post navigation: $e');
      }

      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(
            content: Text('Failed to prepare post for review: $e'),
            duration: const Duration(seconds: 3),
          ),
        );
      }
    }
  }

  void _navigateToHistory() {
    Navigator.push(
      context,
      MaterialPageRoute(
        builder: (context) => const HistoryScreen(),
      ),
    );
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: AnimatedBuilder(
        animation: _backgroundAnimation,
        builder: (context, child) {
          return Container(
            decoration: BoxDecoration(
              gradient: LinearGradient(
                begin: Alignment.topCenter,
                end: Alignment.bottomCenter,
                colors: [
                  Colors.black,
                  Color.lerp(
                    const Color(0xFF1A1A1A),
                    const Color(0xFF2A1A2A),
                    _backgroundAnimation.value * 0.3,
                  )!,
                ],
              ),
            ),
            child: SafeArea(
              child: Stack(
                children: [
                  // Main content
                  _buildMainContent(),

                  // History button (top-right)
                  Positioned(
                    top: 16,
                    right: 16,
                    child: IconButton(
                      onPressed: _navigateToHistory,
                      icon: const Icon(
                        Icons.history,
                        color: Colors.white,
                        size: 28,
                      ),
                    ),
                  ),
                ],
              ),
            ),
          );
        },
      ),
    );
  }

  Widget _buildMainContent() {
    return LayoutBuilder(
      builder: (context, constraints) {
        final screenHeight = constraints.maxHeight;
        final availableHeight =
            screenHeight - 32; // Account for top/bottom padding

        // Calculate proportional heights for the three main sections
        final topSectionHeight =
            screenHeight * 0.12; // Social icons + directory selector
        final bottomSectionHeight =
            screenHeight * 0.25; // Recording area + transcription
        final middleSectionHeight =
            availableHeight - topSectionHeight - bottomSectionHeight;

        return Column(
          children: [
            // Top section - Social Icons + Directory Selector
            SizedBox(
              height: topSectionHeight,
              child: Column(
                children: [
                  // Social Icons with responsive height constraint
                  Expanded(
                    flex: 2,
                    child: Container(
                      padding: const EdgeInsets.symmetric(
                          horizontal: 20, vertical: 8),
                      child: SocialIconsRow(
                        selectedPlatforms: _currentAction?.platforms ?? [],
                        onPlatformToggle: _togglePlatform,
                        maxHeight: topSectionHeight * 0.6,
                      ),
                    ),
                  ),
                  // Directory selector
                  const Expanded(
                    flex: 1,
                    child: DirectorySelector(),
                  ),
                ],
              ),
            ),

            // Middle section - Post Preview with ReviewPostScreen-style content
            SizedBox(
              height: middleSectionHeight,
              child: Container(
                padding:
                    const EdgeInsets.symmetric(horizontal: 20, vertical: 16),
                child: Center(
                  child: Container(
                    constraints: BoxConstraints(
                      maxWidth: constraints.maxWidth * 0.9,
                      maxHeight: middleSectionHeight * 0.95,
                    ),
                    child: _currentAction != null
                        ? _buildReviewStyleContent(context)
                        : _buildEmptyPostPreview(context),
                  ),
                ),
              ),
            ),

            // Bottom section - Transcription + Recording
            SizedBox(
              height: bottomSectionHeight,
              child: Column(
                children: [
                  // Transcription status (upper part of bottom section) - now includes recording timer
                  if (_transcription.isNotEmpty ||
                      _recordingState == RecordingState.processing ||
                      _recordingState == RecordingState.recording)
                    Expanded(
                      flex: 2,
                      child: Container(
                        padding: const EdgeInsets.symmetric(horizontal: 20),
                        child: Center(
                          child: TranscriptionStatus(
                            transcription: _transcription,
                            isProcessing:
                                _recordingState == RecordingState.processing,
                            isRecording:
                                _recordingState == RecordingState.recording,
                            recordingDuration: _recordingDuration,
                            maxRecordingDuration: _maxRecordingDuration,
                            context: _getTranscriptionContext(),
                          ),
                        ),
                      ),
                    )
                  else
                    const Expanded(flex: 2, child: SizedBox()),

                  // Unified action button area (lower part of bottom section)
                  Expanded(
                    flex: 3,
                    child: SafeArea(
                      minimum: const EdgeInsets.only(bottom: 16),
                      child: Center(
                        child: SizedBox(
                          width: bottomSectionHeight * 0.6,
                          height: bottomSectionHeight * 0.6,
                          child: UnifiedActionButton(
                            state: _getUnifiedButtonState(),
                            amplitude: normalizedAmplitude,
                            onRecordStart: _startRecording,
                            onRecordStop: _stopRecording,
                            onReviewPost: _navigateToReviewPost,
                            onAddMedia: _navigateToMediaSelection,
                          ),
                        ),
                      ),
                    ),
                  ),
                ],
              ),
            ),
          ],
        );
      },
    );
  }

  Widget _buildReviewStyleContent(BuildContext context) {
    return Column(
      children: [
        // Main content area (flexible) - removed redundant platform indicators
        Expanded(
          child: SingleChildScrollView(
            physics: const BouncingScrollPhysics(),
            child: Column(
              children: [
                const SizedBox(height: 16),

                // Media preview (if available)
                if (_currentAction!.content.media.isNotEmpty) ...[
                  _buildMediaPreview(context),
                  const SizedBox(height: 16),
                ],

                // Post content box (main text/hashtags)
                PostContentBox(
                  action: _currentAction!,
                  onEditText: null, // Disable editing in command screen
                  onVoiceEdit: null, // Disable voice editing in command screen
                ),

                const SizedBox(height: 16),
              ],
            ),
          ),
        ),
        // Action buttons removed - now handled by UnifiedActionButton
      ],
    );
  }

  Widget _buildEmptyPostPreview(BuildContext context) {
    return Container(
      width: double.infinity, // Full width, edge to edge
      height: 250, // Consistent height with media preview
      decoration: BoxDecoration(
        color:
            Colors.white.withValues(alpha: 0.1), // Dark translucent background
        border: Border.all(
          color: Colors.white.withValues(alpha: 0.3),
          width: 1,
        ),
      ),
      child: const Center(
        child: Column(
          mainAxisAlignment: MainAxisAlignment.center,
          children: [
            Icon(
              Icons.post_add,
              color: Colors.white,
              size: 48,
            ),
            SizedBox(height: 16),
            Text(
              'Your post preview will appear here',
              style: TextStyle(
                color: Colors.white,
                fontSize: 16,
                fontWeight: FontWeight.w500,
              ),
              textAlign: TextAlign.center,
            ),
            SizedBox(height: 8),
            Text(
              'Start recording to create your post',
              style: TextStyle(
                color: Colors.white70,
                fontSize: 14,
              ),
              textAlign: TextAlign.center,
            ),
          ],
        ),
      ),
    );
  }

  Widget _buildMediaPreview(BuildContext context) {
    if (_currentAction!.content.media.isEmpty) return const SizedBox.shrink();

    final mediaItem = _currentAction!.content.media.first;
    final isVideo = mediaItem.mimeType.startsWith('video/');

    return Container(
      width: double.infinity, // Full width, edge to edge
      height: 250, // Increased height to match ReviewPostScreen
      decoration: BoxDecoration(
        color: Colors.white,
        boxShadow: [
          BoxShadow(
            color: Colors.black.withValues(alpha: 0.1),
            blurRadius: 20,
            offset: const Offset(0, 8),
          ),
        ],
      ),
      child: Stack(
        fit: StackFit.expand,
        children: [
          isVideo
              ? Container(
                  color: Colors.grey.shade200,
                  child: const Center(
                    child: Icon(
                      Icons.videocam,
                      color: Colors.grey,
                      size: 40,
                    ),
                  ),
                )
              : Image.file(
                  File(Uri.parse(mediaItem.fileUri).path),
                  fit: BoxFit.cover,
                  errorBuilder: (context, error, stackTrace) {
                    return Container(
                      color: Colors.grey.shade200,
                      child: const Center(
                        child: Icon(
                          Icons.image,
                          color: Colors.grey,
                          size: 40,
                        ),
                      ),
                    );
                  },
                ),

          // Media info overlay
          Positioned(
            bottom: 0,
            left: 0,
            right: 0,
            child: Container(
              padding: const EdgeInsets.all(12),
              decoration: BoxDecoration(
                gradient: LinearGradient(
                  begin: Alignment.bottomCenter,
                  end: Alignment.topCenter,
                  colors: [
                    Colors.black.withValues(alpha: 0.7),
                    Colors.transparent,
                  ],
                ),
              ),
              child: Text(
                '${mediaItem.deviceMetadata.width} √ó ${mediaItem.deviceMetadata.height}',
                style: const TextStyle(
                  color: Colors.white,
                  fontSize: 12,
                  fontWeight: FontWeight.w500,
                ),
              ),
            ),
          ),
        ],
      ),
    );
  }

  Widget _buildActionButtons(BuildContext context) {
    final hasMedia = _currentAction!.content.media.isNotEmpty;
    final hasMediaQuery = _currentAction!.mediaQuery?.isNotEmpty == true;

    return Container(
      padding: const EdgeInsets.all(16),
      decoration: BoxDecoration(
        color: Colors.black.withValues(alpha: 0.3),
        borderRadius: BorderRadius.circular(12),
      ),
      child: Column(
        mainAxisSize: MainAxisSize.min,
        children: [
          // Main action button
          SizedBox(
            width: double.infinity,
            height: 48,
            child: ElevatedButton.icon(
              icon: const Icon(Icons.send, color: Colors.white, size: 18),
              label: const Text(
                'Review Post',
                style: TextStyle(
                  color: Colors.white,
                  fontWeight: FontWeight.w600,
                  fontSize: 16,
                ),
              ),
              onPressed: _navigateToReviewPost,
              style: ElevatedButton.styleFrom(
                backgroundColor: const Color(0xFFFF0080),
                shape: RoundedRectangleBorder(
                  borderRadius: BorderRadius.circular(12),
                ),
                elevation: 2,
              ),
            ),
          ),

          // Secondary button (if needed)
          if (!hasMedia && hasMediaQuery) ...[
            const SizedBox(height: 12),
            SizedBox(
              width: double.infinity,
              height: 40,
              child: OutlinedButton.icon(
                icon: const Icon(Icons.photo_library, size: 16),
                label: const Text(
                  'Add Media',
                  style: TextStyle(fontWeight: FontWeight.w500, fontSize: 14),
                ),
                onPressed: _navigateToMediaSelection,
                style: OutlinedButton.styleFrom(
                  foregroundColor: Colors.white,
                  side: const BorderSide(color: Colors.white),
                  shape: RoundedRectangleBorder(
                    borderRadius: BorderRadius.circular(12),
                  ),
                ),
              ),
            ),
          ],
        ],
      ),
    );
  }

  Color _getPlatformColor(String platform) {
    switch (platform) {
      case 'facebook':
        return Colors.blue.shade700;
      case 'instagram':
        return Colors.pink.shade400;
      case 'twitter':
        return Colors.lightBlue.shade400;
      case 'tiktok':
        return Colors.black87;
      default:
        return Colors.grey.shade600;
    }
  }

  Widget _getPlatformIcon(String platform) {
    switch (platform) {
      case 'facebook':
        return const Icon(Icons.facebook, color: Colors.white, size: 16);
      case 'instagram':
        return const Icon(Icons.camera_alt, color: Colors.white, size: 16);
      case 'twitter':
        return const Icon(Icons.alternate_email, color: Colors.white, size: 16);
      case 'tiktok':
        return const Icon(Icons.music_note, color: Colors.white, size: 16);
      default:
        return const Icon(Icons.share, color: Colors.white, size: 16);
    }
  }

  UnifiedButtonState _getUnifiedButtonState() {
    switch (_recordingState) {
      case RecordingState.idle:
        return UnifiedButtonState.idle;
      case RecordingState.recording:
        return UnifiedButtonState.recording;
      case RecordingState.processing:
        return UnifiedButtonState.processing;
      case RecordingState.ready:
        // Check if we need media selection
        final hasMedia = _currentAction!.content.media.isNotEmpty;
        final hasMediaQuery = _currentAction!.mediaQuery?.isNotEmpty == true;

        if (!hasMedia && hasMediaQuery) {
          return UnifiedButtonState.addMedia;
        }
        // When transcription is ready and media is available, show review post button
        return UnifiedButtonState.reviewPost;
    }
  }

  TranscriptionContext _getTranscriptionContext() {
    switch (_recordingState) {
      case RecordingState.idle:
        return TranscriptionContext.recording;
      case RecordingState.recording:
        return TranscriptionContext.recording;
      case RecordingState.processing:
        return TranscriptionContext.processing;
      case RecordingState.ready:
        // Check if we need media selection
        final hasMedia = _currentAction!.content.media.isNotEmpty;
        final hasMediaQuery = _currentAction!.mediaQuery?.isNotEmpty == true;

        if (!hasMedia && hasMediaQuery) {
          return TranscriptionContext.addMediaReady;
        }
        return TranscriptionContext.reviewReady;
    }
  }
}
